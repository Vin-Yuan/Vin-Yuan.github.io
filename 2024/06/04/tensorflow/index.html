<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="基本元素tf.Variable(……)123456import tensorflow as tfA &#x3D; tf.Variable(3, name&#x3D;&quot;number&quot;)B &#x3D; tf.Variable([1,3], name&#x3D;&quot;vector&quot;)C &#x3D; tf.Variable([[0,1],[2,3]], name&#x3D;&quot;matrix&quot;)D &#x3D; tf.">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow">
<meta property="og:url" content="http://yoursite.com/2024/06/04/tensorflow/index.html">
<meta property="og:site_name" content="Vin&#39;s Blog">
<meta property="og:description" content="基本元素tf.Variable(……)123456import tensorflow as tfA &#x3D; tf.Variable(3, name&#x3D;&quot;number&quot;)B &#x3D; tf.Variable([1,3], name&#x3D;&quot;vector&quot;)C &#x3D; tf.Variable([[0,1],[2,3]], name&#x3D;&quot;matrix&quot;)D &#x3D; tf.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-06-04T07:14:36.000Z">
<meta property="article:modified_time" content="2025-05-22T05:50:40.080Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2024/06/04/tensorflow/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>tensorflow | Vin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/atom.xml" title="Vin's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Vin's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/06/04/tensorflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vin's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          tensorflow
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-06-04 15:14:36" itemprop="dateCreated datePublished" datetime="2024-06-04T15:14:36+08:00">2024-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-05-22 13:50:40" itemprop="dateModified" datetime="2025-05-22T13:50:40+08:00">2025-05-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="基本元素"><a href="#基本元素" class="headerlink" title="基本元素"></a>基本元素</h2><h3 id="tf-Variable-……"><a href="#tf-Variable-……" class="headerlink" title="tf.Variable(……)"></a>tf.Variable(……)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">A = tf.Variable(<span class="number">3</span>, name=<span class="string">&quot;number&quot;</span>)</span><br><span class="line">B = tf.Variable([<span class="number">1</span>,<span class="number">3</span>], name=<span class="string">&quot;vector&quot;</span>)</span><br><span class="line">C = tf.Variable([[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">3</span>]], name=<span class="string">&quot;matrix&quot;</span>)</span><br><span class="line">D = tf.Variable(tf.zeros([<span class="number">100</span>]), name=<span class="string">&quot;zero&quot;</span>)</span><br><span class="line">E = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>], mean=<span class="number">1</span>, stddev=<span class="number">2</span>, dtype=tf.float32))</span><br></pre></td></tr></table></figure>
<p>我们可以把函数variable()理解为构造函数，构造函数的使用需要初始值，而这个初始值是一个任何形状、类型的Tensor。<br>变量有两个重要的步骤，先后为：</p>
<ul>
<li>创建</li>
<li>初始化</li>
</ul>
<p>变量在使用前一定要进行初始化，且变量的初始化必须在模型的其它操作运行之前完成，通常，变量的初始化有三种方式：</p>
<ul>
<li>1.初始化全部变量<br><code>init = tf.global_variables_initializer()</code><br>global_variables_initializer()方法是不管全局有多少个变量，全部进行初始化，是最简单也是最常用的一种方式；</li>
<li>2.初始化变量的子集<br><code>init_subset=tf.variables_initializer([b,c], name=&quot;init_subset&quot;)</code><br>variables_initializer()是初始化变量的子集，相比于全部初始化化的方式更加节约内存</li>
<li>3.初始化单个变量</li>
</ul>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nit_var = tf.Variable(tf.zeros([<span class="number">2</span>,<span class="number">5</span>]))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_var.initializer)</span><br></pre></td></tr></table></figure>
<p>Variable()是初始化单个变量，函数的参数便是要初始化的变量内容。</p>
<h3 id="为什么要使用tf-global-variables-initializer-？"><a href="#为什么要使用tf-global-variables-initializer-？" class="headerlink" title="为什么要使用tf.global_variables_initializer()？"></a>为什么要使用tf.global_variables_initializer()？</h3><p>参考博客<a href="https://blog.csdn.net/qq_37285386/article/details/89054090">【任意门】</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 必须要使用global_variables_initializer的场合</span></span><br><span class="line"><span class="comment"># 含有tf.Variable的环境下，因为tf中建立的变量是没有初始化的，也就是在debug时还不是一个tensor量，而是一个Variable变量类型</span></span><br><span class="line">size_out = <span class="number">10</span></span><br><span class="line">tensor = tf.Variable(tf.random_normal(shape=[size_out]))</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)  <span class="comment"># initialization variables</span></span><br><span class="line">    <span class="built_in">print</span>(sess.run(tensor))</span><br><span class="line"><span class="comment"># 可以不适用初始化的场合</span></span><br><span class="line"><span class="comment"># 不含有tf.Variable、tf.get_Variable的环境下</span></span><br><span class="line"><span class="comment"># 比如只有tf.random_normal或tf.constant等</span></span><br><span class="line">size_out = <span class="number">10</span></span><br><span class="line">tensor = tf.random_normal(shape=[size_out])  <span class="comment"># 这里debug是一个tensor量哦</span></span><br><span class="line"><span class="comment">#init = tf.global_variables_initializer()</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># sess.run(init)  # initialization variables</span></span><br><span class="line">    <span class="built_in">print</span>(sess.run(tensor))</span><br></pre></td></tr></table></figure>
<p>需要注意的是 tf.placeholder也是tensor，可以这样理解，tf.Variable是需要申请存储（显存/内存）的变量，而tensor:</p>
<ol>
<li>计算图上计算的中间结果，比如operation</li>
<li>常量，比如tf.random_normal, tf.constant</li>
<li>等待输入的placeholder（不需要初始化，等待feed data)<br>常见的计算系统，无非是操作数，运算符，然后是存储器，如果施加运算符的步骤不再立刻执行，而是最后计算，那么这些中构结果就没必要一开始申请存储，这便是tensor的由来。</li>
</ol>
<h3 id="获取graph的名称"><a href="#获取graph的名称" class="headerlink" title="获取graph的名称"></a>获取graph的名称</h3><p>参考<a href="https://stackoverflow.com/questions/36883949/in-tensorflow-get-the-names-of-all-the-tensors-in-a-graph">stackoverflow</a></p>
<ul>
<li>To get all nodes:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_nodes = [n <span class="keyword">for</span> n <span class="keyword">in</span> tf.get_default_graph().as_graph_def().node]</span><br><span class="line">These have the <span class="built_in">type</span> tensorflow.core.framework.node_def_pb2.NodeDef</span><br></pre></td></tr></table></figure>
<ul>
<li>To get all ops:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_ops = tf.get_default_graph().get_operations()</span><br><span class="line">These have the <span class="built_in">type</span> tensorflow.python.framework.ops.Operation</span><br></pre></td></tr></table></figure>
<ul>
<li>To get all variables:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_vars = tf.global_variables()</span><br><span class="line">These have the <span class="built_in">type</span> tensorflow.python.ops.resource_variable_ops.ResourceVariable</span><br></pre></td></tr></table></figure>
<ul>
<li>And finally, to answer the question, to get all tensors:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_tensors = [tensor <span class="keyword">for</span> op <span class="keyword">in</span> tf.get_default_graph().get_operations() <span class="keyword">for</span> tensor <span class="keyword">in</span> op.values()]</span><br></pre></td></tr></table></figure>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="tf-reduce-sum的理解"><a href="#tf-reduce-sum的理解" class="headerlink" title="tf.reduce_sum的理解"></a>tf.reduce_sum的理解</h3><p><a href="https://www.jianshu.com/p/30b40b504bae">https://www.jianshu.com/p/30b40b504bae</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_sum(</span><br><span class="line">    input_tensor, </span><br><span class="line">    axis=None, </span><br><span class="line">    keepdims=None,</span><br><span class="line">    name=None,</span><br><span class="line">    reduction_indices=None, </span><br><span class="line">    keep_dims=None)</span><br></pre></td></tr></table></figure>
<ul>
<li>0维，又称0维张量，数字，标量：1</li>
<li>1维，又称1维张量，数组，vector：[1, 2, 3]</li>
<li>2维，又称2维张量，矩阵，二维数组：[[1,2], [3,4]]</li>
<li>3维，又称3维张量，立方（cube），三维数组：[ [[1,2], [3,4]], [[5,6], [7,8]] ]</li>
<li>n维：你应该get到点了吧~</li>
</ul>
<p><strong>再多的维只不过是是把上一个维度当作自己的元素</strong><br><strong>越往里axis就越大，依次加1</strong><br>下面举个多维tensor例子简单说明。下面是个 2 <em>3</em> 4 的tensor。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[[ 1   2   3   4]</span><br><span class="line">  [ 5   6   7   8]</span><br><span class="line">  [ 9   10 11 12]],</span><br><span class="line"> [[ 13  14 15 16]</span><br><span class="line">  [ 17  18 19 20]</span><br><span class="line">  [ 21  22 23 24]]]</span><br></pre></td></tr></table></figure>
<p>tf.reduce_sum(tensor, axis=0) axis=0 说明是按第一个维度进行求和。那么求和结果shape是3*4</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[1+13   2+14   3+15 4+16]</span><br><span class="line"> [5+17   6+18   7+19 8+20]</span><br><span class="line"> [9+21 10+22 11+23 12+24]]</span><br></pre></td></tr></table></figure>
<p>依次类推，如果axis=1，那么求和结果shape是2*4，即：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[ 1 + 5 + 9   2 + 6+10   3 + 7+11   4 + 8+12]</span><br><span class="line"> [13+17+21     14+18+22   15+19+23   16+20+24]]</span><br></pre></td></tr></table></figure>
<p>如果axis=2，那么求和结果shape是2*3，即：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[1+2+3+4          5+6+7+8          9+10+11+12]</span><br><span class="line"> [13+14+15+16      17+18+19+20      1+22+23+24]]</span><br></pre></td></tr></table></figure>
<h3 id="tf-stack的理解"><a href="#tf-stack的理解" class="headerlink" title="tf.stack的理解"></a>tf.stack的理解</h3><p><a href="https://stackoverflow.com/questions/50820781/quesion-about-the-axis-of-tf-stack/50821422#50821422">https://stackoverflow.com/questions/50820781/quesion-about-the-axis-of-tf-stack/50821422#50821422</a><br>tf.stack可以理解为先对需要stack的tensor做expand_dims，添加一维，添加的位置即axis，然后在这一axis上做concate</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def tf.stack(tensors, axis=0):</span><br><span class="line">    return tf.concatenate([tf.expand_dims(t, axis=axis) for t in tensors], axis=axis)</span><br></pre></td></tr></table></figure>
<h3 id="具有先后顺序，synchronize的计算"><a href="#具有先后顺序，synchronize的计算" class="headerlink" title="具有先后顺序，synchronize的计算"></a>具有先后顺序，synchronize的计算</h3><ul>
<li>tf.GraphKeys.UPDATE_OPS</li>
<li>tf.control_dependencies</li>
</ul>
<p><a href="https://blog.csdn.net/huitailangyz/article/details/85015611">参考资料</a><br><code>tf.GraphKeys.UPDATE_OPS</code> 和 <code>tf.control_dependencies</code> 搭配使用，用来限制一些有先后关系的节点运算<br><code>tf.control_dependencies</code>，该函数保证其<strong>作用域中的操作</strong>必须要在该函数所传递的<strong>参数中的操作</strong>完成后再进行，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一个运算</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a_1 = tf.Variable(<span class="number">1</span>)</span><br><span class="line">b_1 = tf.Variable(<span class="number">2</span>)</span><br><span class="line">update_op = tf.assign(a_1, <span class="number">10</span>)</span><br><span class="line">add = tf.add(a_1, b_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二个运算</span></span><br><span class="line">a_2 = tf.Variable(<span class="number">1</span>)</span><br><span class="line">b_2 = tf.Variable(<span class="number">2</span>)</span><br><span class="line">update_op = tf.assign(a_2, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([update_op]):</span><br><span class="line">    add_with_dependencies = tf.add(a_2, b_2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    ans_1, ans_2 = sess.run([add, add_with_dependencies])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Add: &quot;</span>, ans_1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Add_with_dependency: &quot;</span>, ans_2)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Add:  <span class="number">3</span></span><br><span class="line">Add_with_dependency:  <span class="number">12</span></span><br></pre></td></tr></table></figure>
<p>可以看到上面例子中，第一个update_op 对变量做了加一操作，<br><strong>但正常的计算图在计算add时是不会经过update_op操作</strong>，所以没有生效。<br>于tf.GraphKeys.UPDATE_OPS，这是一个tensorflow的计算图中内置的一个集合，其中会保存一些需要在训练操作之前完成的操作，并配合tf.control_dependencies函数使用。<br>至于<code>tf.GraphKeys.UPDATE_OPS</code>的作用，可以在Batch Normalization的例子中理解其作用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, train_mean)</span><br><span class="line">……</span><br><span class="line">update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line"><span class="built_in">print</span>(update_ops)</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies(update_ops):</span><br><span class="line">    train_op = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure>
<p>两个<code>tf.add_to_collection</code>在这里是将需要先计算的Mean和var加入UPDATE_OPS中，这样</p>
<blockquote>
<p>如果不在使用时添加tf.control_dependencies函数，即在训练时(training=True)每批次时只会计算当批次的mean和var，并传递给tf.nn.batch_normalization进行归一化，由于mean_update和variance_update在计算图中并不在上述操作的依赖路径上，因为并不会主动完成，也就是说，在训练时mean_update和variance_update并不会被使用到，其值sfsfafsfafafdafa一直是初始值。因此在测试阶段(training=False)使用这两个作为mean和variance并进行归一化操作，这样就会出现错误。而如果使用tf.control_dependencies函数，会在训练阶段每次训练操作执行前被动地去执行mean_update和variance_update，因此moving_mean和moving_variance会被不断更新，在测试时使用该参数也就不会出现错误。</p>
</blockquote>
<h3 id="embedding-和-lookupTable-link"><a href="#embedding-和-lookupTable-link" class="headerlink" title="embedding 和 lookupTable [link]"></a>embedding 和 lookupTable [<a href="https://gshtime.github.io/2018/06/01/tensorflow-embedding-lookup-sparse/">link</a>]</h3><p>feature_num : 原始特征数<br>embedding_size: embedding之后的特征数<br>[feature_num, embedding_size] 权重矩阵shape<br>[m, feature_num] 输入矩阵shape，m为样本数<br>[m, embedding_size] 输出矩阵shape，m为样本数</p>
<p>embedding_lookup不是简单的查表，<a href="https://gshtime.github.io/2018/06/01/tensorflow-embedding-lookup-sparse/">params 对应的向量是可以训练的</a>，训练参数个数应该是 feature_num * embedding_size，即前文表述的embedding层权重矩阵，就是说 lookup 的是一种全连接层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当输入单个tensor时，partition_strategy不起作用，不做 id（编号） 的切分</span></span><br><span class="line">a = np.arange(<span class="number">20</span>).reshape(<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span> (a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前面的编号是我手动加的，意思是不做切分的时候就顺序编号就行</span></span><br><span class="line"><span class="comment"># 0#[[ 0  1  2  3]</span></span><br><span class="line"><span class="comment"># 1# [ 4  5  6  7]</span></span><br><span class="line"><span class="comment"># 2# [ 8  9 10 11]</span></span><br><span class="line"><span class="comment"># 3# [12 13 14 15]</span></span><br><span class="line"><span class="comment"># 4# [16 17 18 19]]</span></span><br><span class="line"></span><br><span class="line">tensor_a = tf.Variable(a)</span><br><span class="line">embedded_tensor = tf.nn.embedding_lookup(params=tensor_a, ids=[<span class="number">0</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    embedded_tensor = sess.run(embedded_tensor)</span><br><span class="line">    <span class="built_in">print</span>(embedded_tensor)</span><br><span class="line"><span class="comment"># 根据 ids 参数做选择</span></span><br><span class="line"><span class="comment">#[[ 0  1  2  3]  选择了 id 0</span></span><br><span class="line"><span class="comment"># [12 13 14 15]  选择了 id 3</span></span><br><span class="line"><span class="comment"># [ 8  9 10 11]  选择了 id 2</span></span><br><span class="line"><span class="comment"># [ 4  5  6  7]] 选择了 id 1</span></span><br></pre></td></tr></table></figure>
<h3 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h3><p><a href="https://zhuanlan.zhihu.com/p/44216830">https://zhuanlan.zhihu.com/p/44216830</a></p>
<h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><p>tf.losses.mean_squared_error<br>tf.losses.absolute_difference<br>tf.losses.huber_loss：Huber loss</p>
<h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>tf.nn.sigmoid_cross_entropy_with_logits<br>tf.losses.log_loss<br>tf.nn.softmax_cross_entropy_with_logits_v2<br>tf.nn.sparse_softmax_cross_entropy_with_logits<br>tf.nn.weighted_cross_entropy_with_logits<br>tf.losses.hinge_loss</p>
<h5 id="tf-softmax-cross-entropy-with-logits"><a href="#tf-softmax-cross-entropy-with-logits" class="headerlink" title="tf.softmax_cross_entropy_with_logits"></a>tf.softmax_cross_entropy_with_logits</h5><p>tf.softmax_cross_entropy_with_logits()的计算过程一共分为两步:</p>
<ul>
<li>1）将logits转换成概率 <script type="math/tex">l_k = \frac{e^k}{\sum_{i=1}^{n}{e^i}}</script></li>
<li>2）计算交叉熵损失 <script type="math/tex">-\Sigma y'* log(y)</script></li>
</ul>
<p>注意事项：<br>般训练时batch size不会为设为1,所以要使用tf.reduce_mean()来对tf.softmax_cross_entropy_with_logits()的结果取平均,得到关于样本的平均交叉熵损失.</p>
<h2 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h2><h3 id="tf-Print"><a href="#tf-Print" class="headerlink" title="tf.Print"></a>tf.Print</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">……</span><br><span class="line">var = tf.concat([var_a, var_b])</span><br><span class="line">var = tf.Print(var, [var_a, var_b], message=&quot;print message in there&quot;, summarized=10000)</span><br></pre></td></tr></table></figure>
<p>tf.Print 类似identity，挂载到图上，但不影响图结构，所以即使是checkpoint也可以打印计算的中间结果，方便诊断问题。需要注意的是待打印的变量需是图中流过var的上端节点tensor</p>
<h3 id="tf-cond"><a href="#tf-cond" class="headerlink" title="tf.cond"></a>tf.cond</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.constant(2)</span><br><span class="line">b = tf.constant(3)</span><br><span class="line">x = tf.constant(4)</span><br><span class="line">y = tf.constant(5)</span><br><span class="line">z = tf.multiply(a, b)</span><br><span class="line">result = tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y))</span><br><span class="line">with tf.Session() as session:</span><br><span class="line">    print(result.eval())</span><br></pre></td></tr></table></figure>
<h2 id="same-和-padding"><a href="#same-和-padding" class="headerlink" title="same 和 padding"></a>same 和 padding</h2><script type="math/tex; mode=display">
\left\lceil\frac{n_{i}+p_{i}-k+1}{s}\right\rceil=n_{o}</script><p>$n_i$ 为input_size<br>$n_o$ 为output_size<br>$k$   为 kernel size<br>$s$   为 stride<br>$p_i$ 为 padding size<br><a href="https://www.jianshu.com/p/b9eb4758118d">https://www.jianshu.com/p/b9eb4758118d</a></p>
<h2 id="Session-run-a-b-c-中变量的顺序"><a href="#Session-run-a-b-c-中变量的顺序" class="headerlink" title="Session.run([a,b,c])中变量的顺序"></a>Session.run([a,b,c])中变量的顺序</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_val, _ = sess.run([loss, optimizer])</span><br></pre></td></tr></table></figure>
<p>对于上面遇到的问题，可能会产生怀疑，这个Loss到底是back propgation之前的还是之后的？在查看<a href="https://stackoverflow.com/questions/53165418/order-of-sess-runop1-op2-in-tensorflow">stack overflow</a> 上的解答，发现sess.run中的变量求解是不确定的。上面的代码求解的是BP之前的，tensorflow为了保证不重复计算，图中节点已经计算过的会直接取出，若想获取BP之后的Loss, 可通过如下方式：</p>
<ol>
<li>再次sess.run([loss])</li>
<li>定义一个loss_end tensor,</li>
<li>使用tf.control_dependencies([optimizer])来规定依赖</li>
</ol>
<h2 id="Session-和-Graph-的关系"><a href="#Session-和-Graph-的关系" class="headerlink" title="Session 和 Graph 的关系"></a>Session 和 Graph 的关系</h2><p><a href="https://www.easy-tensorflow.com/tf-tutorials/basics/graph-and-session">https://www.easy-tensorflow.com/tf-tutorials/basics/graph-and-session</a><br>网络经过定义然后训练后得的参数保存在session中而非graph中，graph只是网路结构的表述。</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p><a href="https://zhuanlan.zhihu.com/p/44216830">https://zhuanlan.zhihu.com/p/44216830</a></p>
<h3 id="1-tf-nn-sigmoid-cross-entropy-with-logits"><a href="#1-tf-nn-sigmoid-cross-entropy-with-logits" class="headerlink" title="1. tf.nn.sigmoid_cross_entropy_with_logits"></a>1. tf.nn.sigmoid_cross_entropy_with_logits</h3><p>先 sigmoid 再求交叉熵,二分类问题首选,使用时，一定不要将预测值（y_pred）进行 sigmoid 处理，因为这个函数已经包含了sigmoid过程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Tensorflow中集成的函数</span><br><span class="line">sigmoids = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_pred)</span><br><span class="line">sigmoids_loss = tf.reduce_mean(sigmoids)</span><br><span class="line"></span><br><span class="line"># 利用Tensorflow基础函数手工实现</span><br><span class="line">y_pred_si = 1.0/(1+tf.exp(-y_pred))</span><br><span class="line">sigmoids = -y_true*tf.log(y_pred_si) - (1-y_true)*tf.log(1-y_pred_si)</span><br><span class="line">sigmoids_loss = tf.reduce_mean(sigmoids)</span><br></pre></td></tr></table></figure>
<h3 id="2-tf-losses-log-loss"><a href="#2-tf-losses-log-loss" class="headerlink" title="2. tf.losses.log_loss"></a>2. tf.losses.log_loss</h3><p>预测值（y_pred）计算完成后，若已先行进行了 sigmoid 处理，则使用此函数求 loss ，若还没经过 sigmoid 处理，可直接使用 sigmoid_cross_entropy_with_logits。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Tensorflow中集成的函数</span><br><span class="line">logs = tf.losses.log_loss(labels=y, logits=y_pred)</span><br><span class="line">logs_loss = tf.reduce_mean(logs)</span><br><span class="line"></span><br><span class="line"># 利用Tensorflow基础函数手工实现</span><br><span class="line">logs = -y_true*tf.log(y_pred) - (1-y_true)*tf.log(1-y_pred)</span><br><span class="line">logs_loss = tf.reduce_mean(logs)</span><br></pre></td></tr></table></figure>
<h2 id="模型导出和恢复"><a href="#模型导出和恢复" class="headerlink" title="模型导出和恢复"></a>模型导出和恢复</h2><p><a href="https://www.jianshu.com/p/c3a7f5c47b83">TensorFlow：保存和提取模型</a></p>
<h3 id="模型恢复"><a href="#模型恢复" class="headerlink" title="模型恢复"></a>模型恢复</h3><h4 id="savedModel"><a href="#savedModel" class="headerlink" title="savedModel"></a>savedModel</h4><p><a href="https://blog.csdn.net/mogoweb/article/details/83054861">如何查看Tensorflow SavedModel格式模型的信息</a><br>signature并非是为了保证模型不被修改的那种电子签名。类似于编程语言中模块的输入输出信息，比如函数名，输入参数类型，输出参数类型等等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> gfile</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.core.protobuf <span class="keyword">import</span> saved_model_pb2</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.util <span class="keyword">import</span> compat</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  model_filename =<span class="string">&#x27;./model/saved_model.pb&#x27;</span></span><br><span class="line">  <span class="keyword">with</span> gfile.FastGFile(model_filename, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line"></span><br><span class="line">    data = compat.as_bytes(f.read())</span><br><span class="line">    sm = saved_model_pb2.SavedModel()</span><br><span class="line">    sm.ParseFromString(data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="number">1</span> != <span class="built_in">len</span>(sm.meta_graphs):</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&#x27;More than one graph found. Not sure which to write&#x27;</span>)</span><br><span class="line">      sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    g_in = tf.import_graph_def(sm.meta_graphs[<span class="number">0</span>].graph_def)</span><br><span class="line">LOGDIR=<span class="string">&#x27;./logdir&#x27;</span></span><br><span class="line">train_writer = tf.summary.FileWriter(LOGDIR)</span><br><span class="line">train_writer.add_graph(sess.graph)</span><br><span class="line">train_writer.flush()</span><br><span class="line">train_writer.close()</span><br></pre></td></tr></table></figure>
<p>另外可参考stackoverflow的总结<br><a href="https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model">https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model</a></p>
<h2 id="Tensorflow-主流程"><a href="#Tensorflow-主流程" class="headerlink" title="Tensorflow 主流程"></a>Tensorflow 主流程</h2><h3 id="梯度更新部分"><a href="#梯度更新部分" class="headerlink" title="梯度更新部分"></a>梯度更新部分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.control_dependencies(update_ops):</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate)</span><br><span class="line"><span class="comment"># 计算梯度，grads_and_vars 是一个list：(gradient, variable)，变量和变量对应的梯度    </span></span><br><span class="line">grads_and_vars = optimizer.compute_gradients(loss)</span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># ...... (此处可以做 梯度修建等操作，然后再对变量更新梯度）</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 执行对应变量的更新梯度操作</span></span><br><span class="line">train_op = optimizer.apply_gradients(grad_vars, global_step=global_step)</span><br></pre></td></tr></table></figure>
<h3 id="tf-Summary"><a href="#tf-Summary" class="headerlink" title="tf.Summary"></a>tf.Summary</h3><p><a href="https://zhuanlan.zhihu.com/p/31459527">https://zhuanlan.zhihu.com/p/31459527</a><br><a href="https://www.cnblogs.com/lyc-seu/p/8647792.html">https://www.cnblogs.com/lyc-seu/p/8647792.html</a></p>
<h2 id="Keras-backend-tensorflow"><a href="#Keras-backend-tensorflow" class="headerlink" title="Keras backend tensorflow"></a>Keras backend tensorflow</h2><p>keras 和 tensorflow-gpu版本兼容列表<br><a href="https://docs.floydhub.com/guides/environments/">https://docs.floydhub.com/guides/environments/</a><br>应对不同tensorlow-gpu好cuda版本的安装<br><code>conda install tensorflow-gpu==1.9.0 cudatoolkit==8.0</code></p>
<h2 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h2><p>多个model对比train, validation效果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=name1:/path/to/logs/1,name2:/path/to/logs/2</span><br></pre></td></tr></table></figure>
<p>demo 例子: <a href="https://blog.csdn.net/qiu931110/article/details/80137287">https://blog.csdn.net/qiu931110/article/details/80137287</a><br>讲解：<a href="https://cloud.tencent.com/developer/section/1475708">https://cloud.tencent.com/developer/section/1475708</a></p>
<h1 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h1><h2 id="分布式-1"><a href="#分布式-1" class="headerlink" title="分布式"></a>分布式</h2><p><a href="https://yq.aliyun.com/articles/603370">参考资料:浅显易懂的分布式TensorFlow入门教程</a></p>
<h3 id="系统会包含三种类型的节点"><a href="#系统会包含三种类型的节点" class="headerlink" title="系统会包含三种类型的节点"></a>系统会包含三种类型的节点</h3><ul>
<li><strong>一个或多个参数服务器（ps server)</strong>，用来存放模型</li>
<li><strong>一个主worker</strong>，用来协调训练操作，负责模型的初始化，为训练步骤计数，保存模型到checkpoints中，从checkpoints中读取模型，向TensorBoard中保存summaries（需要展示的信息）。主worker还要负责分布式计算的容错机制（如果参数服务器或worker服务器崩溃）。</li>
<li><strong>worker服务器（包括主worker服务器）</strong>，用来执行训练操作，并向参数服务器发送更新操作。(worker服务器在这里是指多个worker节点，集群的意思，见上面结构图）</li>
</ul>
<blockquote>
<p>也就是说最小的集群需要包含一个主worker服务器和一个参数服务器。可以将它扩展为一个主worker服务器，多个参数服务器和多个worker服务器。<br>最好有多个参数服务器，<strong>因为worker服务器和参数服务器之间有大量的I/O通信</strong>。如果只有2个worker服务器，可能1个参数服务器可以扛得住所有的读取和更新请求。但如果你有10个worker而且你的模型非常大，一个参数服务器可能就不够了。</p>
</blockquote>
<p><strong>在分布式TensorFlow中，同样的代码会被发送到所有的节点</strong>。虽然你的main.py、train.py等会被同时发送到worker服务器和参数服务器，但<br>每个节点会依据自己的环境变量来执行不同的代码块。</p>
<h3 id="分布式TensorFlow代码的准备包括三个阶段"><a href="#分布式TensorFlow代码的准备包括三个阶段" class="headerlink" title="分布式TensorFlow代码的准备包括三个阶段"></a>分布式TensorFlow代码的准备包括三个阶段</h3><ol>
<li>定义tf.trainClusterSpec和tf.train.Server</li>
<li>将模型赋给参数服务器和worker服务器</li>
<li>配置和启动tf.train.MonitoredTrainingSession</li>
</ol>
<h2 id="PS-and-Worker"><a href="#PS-and-Worker" class="headerlink" title="PS and Worker"></a>PS and Worker</h2><p>参考:<br><a href="https://zhuanlan.zhihu.com/p/35083779">【1】分布式TensorFlow入门教程</a><br><a href="https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md">【2】Distributed TensorFlow</a><br><a href="https://www.oreilly.com/content/distributed-tensorflow/">【3】Distributed TensorFlow</a></p>
<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><p>A client is typically a program that builds a TensorFlow graph and constructs a tensorflow::Session to interact with a cluster. Clients are typically written in Python or C++. <strong>A single client process can directly interact with multiple TensorFlow servers</strong> (see “Replicated training” above), <strong>and a single server can serve multiple clients</strong>.<br>server在这里是服务者的角色，无论是ps还是worker都是server,我们可以建立多个server,服务与多个client。比如有个worker server已经建立，A用client建立一个regression任务是使用这个server训练，B用client建立了一个CNN任务也使用了这个server，这不冲突，在资源充足情况下是可以先后使用同一个server的。</p>
<h3 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h3><p>A job comprises a list of “tasks”, which typically serve a common purpose. For example, a job named ps (for “parameter server”) typically hosts nodes that store and update variables; while a job named worker typically hosts stateless nodes that perform compute-intensive tasks. The tasks in a job typically run on different machines. The set of job roles is flexible: for example, a worker may maintain some state.<br>注意原则上ps和worker两种job功能不同</p>
<ul>
<li>ps (parameter server)：hosts nodes that <strong>store</strong> and <strong>update</strong> variables;</li>
<li>worker：hosts <strong>stateless</strong> nodes that perform <strong>compute-intensive tasks</strong>.<br>虽然规则上这样各司其职，但实际上并不一定需要严格这样执行，job的角色是灵活的，比如，worker也可以维护一些状态（state)<h3 id="Master-service"><a href="#Master-service" class="headerlink" title="Master service"></a>Master service</h3>An RPC service that provides remote access to a set of distributed devices, and <strong>acts as a session target.</strong> The master service implements the tensorflow::Session interface, and is responsible for coordinating work across one or more “worker services”. <strong>All TensorFlow servers implement the master service.</strong><br>具体理解参照下面的例子：</li>
</ul>
<p>在【1】的例子中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># example.py</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">tf.app.flags.DEFINE_string(&quot;ps_hosts&quot;, &quot;localhost:2222&quot;, &quot;ps hosts&quot;)</span><br><span class="line">tf.app.flags.DEFINE_string(&quot;worker_hosts&quot;, &quot;localhost:2223,localhost:2224&quot;, &quot;worker hosts&quot;)</span><br><span class="line">tf.app.flags.DEFINE_string(&quot;job_name&quot;, &quot;worker&quot;, &quot;&#x27;ps&#x27; or&#x27;worker&#x27;&quot;)</span><br><span class="line">tf.app.flags.DEFINE_integer(&quot;task_index&quot;, 0, &quot;Index of task within the job&quot;)</span><br><span class="line"></span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line"></span><br><span class="line">def main(_):</span><br><span class="line">    ps_hosts = FLAGS.ps_hosts.split(&quot;,&quot;)</span><br><span class="line">    worker_hosts = FLAGS.worker_hosts.split(&quot;,&quot;)</span><br><span class="line">    # create cluster</span><br><span class="line">    cluster = tf.train.ClusterSpec(&#123;&quot;ps&quot;: ps_hosts, &quot;worker&quot;: worker_hosts&#125;)</span><br><span class="line">    # create the server</span><br><span class="line">    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)</span><br><span class="line">    server.join()</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>
<p>exmple.py用来建立执行不同功能的server，执行上面的example.py来生成不同的<strong>server</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python example.py --job_name=ps --task_index=0</span><br><span class="line">python example.py --job_name=worker --task_index=0</span><br><span class="line">python example.py --job_name=worker --task_index=1</span><br></pre></td></tr></table></figure>
<p>以work-0为例子，打印日志如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2020-03-20 15:50:24.761196: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job ps -&gt; &#123;0 -&gt; localhost:2222&#125;</span><br><span class="line">2020-03-20 15:50:24.761240: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -&gt; &#123;0 -&gt; localhost:2223, 1 -&gt; localhost:2224&#125;</span><br><span class="line">2020-03-20 15:50:24.762191: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:2223</span><br></pre></td></tr></table></figure>
<p>日志里显示了当前server的grpc地址，以及server所知道的其他集群信息，这是理所应当，如果我们需要协调分布式任务，必然需要知道其他服务的信息，这样才可以通信协调工作。<br>到目前为止，server已经建立，也就意味着资源已经建立，而接下来我们就可以通过client使用这些资源来完成分布式任务了。<br>我们创建一个client来执行一个计算图，并且采用/job:worker/task:0这个server所对应的<strong>master</strong>，即grpc://localhost:2223来创建Session，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#client.py</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    with tf.device(&quot;/job:ps/task:0&quot;):</span><br><span class="line">        x = tf.Variable(tf.ones([2, 2]))</span><br><span class="line">        y = tf.Variable(tf.ones([2, 2]))</span><br><span class="line"></span><br><span class="line">    with tf.device(&quot;/job:worker/task:0&quot;):</span><br><span class="line">        z = tf.matmul(x, y) + x</span><br><span class="line"></span><br><span class="line">    with tf.device(&quot;/job:worker/task:1&quot;):</span><br><span class="line">        z = tf.matmul(z, x) + x</span><br><span class="line"></span><br><span class="line">    with tf.Session(&quot;grpc://localhost:2223&quot;) as sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        val = sess.run(z)</span><br><span class="line">        print(val)</span><br></pre></td></tr></table></figure>
<p>其实这个client就是一个进程，但是其在计算时需要依靠cluster中的device来执行部分计算子图。这时候各个server的日志中，只有2223的日志发生了变化，多了一行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">……</span><br><span class="line">2020-03-20 15:55:50.796022: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session b7f2e9eb1f8c5548 with config:</span><br></pre></td></tr></table></figure>
<h2 id="Between-graph-replication"><a href="#Between-graph-replication" class="headerlink" title="Between-graph replication"></a>Between-graph replication</h2><p>在Between-graph replication中，各个worker都包含一个client，它们构建相同的计算图，然后把参数放在ps上，TensorFlow提供了一个专门的函数tf.train.replica_device_setter来方便Graph构建，先看代码【1】：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># cluster包含两个ps 和三个 worker</span><br><span class="line">cluster_spec = &#123;</span><br><span class="line">    &quot;ps&quot;: [&quot;ps0:2222&quot;, &quot;ps1:2222&quot;],</span><br><span class="line">    &quot;worker&quot;: [&quot;worker0:2222&quot;, &quot;worker1:2222&quot;, &quot;worker2:2222&quot;]&#125;</span><br><span class="line">cluster = tf.train.ClusterSpec(cluster_spec)</span><br><span class="line">with tf.device(tf.train.replica_device_setter(</span><br><span class="line">                worker_device=&quot;/job:worker/task:%d&quot; % FLAGS.task_index,</span><br><span class="line">               cluster=cluster)):</span><br><span class="line">  # Build your graph</span><br><span class="line">  v1 = tf.Variable(...)  # assigned to /job:ps/task:0</span><br><span class="line">  v2 = tf.Variable(...)  # assigned to /job:ps/task:1</span><br><span class="line">  v3 = tf.Variable(...)  # assigned to /job:ps/task:0</span><br><span class="line">  # Run compute</span><br></pre></td></tr></table></figure>
<p>使用<strong>tf.train.replica_device_setter</strong>可以自动把Graph中的Variables放到ps上，而同时将Graph的计算部分放置在当前worker上，省去了很多麻烦。由于ps往往不止一个，这个函数在为各个Variable分配ps时默认采用简单的round-robin方式，就是按次序将参数挨个放到各个ps上，但这个方式可能不能使ps负载均衡，如果需要更加合理，可以采用tf.contrib.training.GreedyLoadBalancingStrategy策略。<br>采用Between-graph replication方式的另外一个问题，由于各个worker都独立拥有自己的client，但是对于一些公共操作比如模型参数初始化与checkpoint文件保存等，如果每个client都独立进行这些操作，显然是对资源的浪费。为了解决这个问题，一般会指定一个worker为chief worker，它将作为各个worker的管家，协调它们之间的训练，并且完成模型初始化和模型保存和恢复等公共操作。在TensorFlow中，可以使用tf.train.MonitoredTrainingSession创建client的Session，并且其可以指定哪个worker是chief worker。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/06/04/recommendation-system/" rel="prev" title="recommendation_system">
      <i class="fa fa-chevron-left"></i> recommendation_system
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/06/04/photograpy/" rel="next" title="photograpy">
      photograpy <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%83%E7%B4%A0"><span class="nav-number">1.</span> <span class="nav-text">基本元素</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-Variable-%E2%80%A6%E2%80%A6"><span class="nav-number">1.1.</span> <span class="nav-text">tf.Variable(……)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8tf-global-variables-initializer-%EF%BC%9F"><span class="nav-number">1.2.</span> <span class="nav-text">为什么要使用tf.global_variables_initializer()？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96graph%E7%9A%84%E5%90%8D%E7%A7%B0"><span class="nav-number">1.3.</span> <span class="nav-text">获取graph的名称</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-reduce-sum%E7%9A%84%E7%90%86%E8%A7%A3"><span class="nav-number">2.1.</span> <span class="nav-text">tf.reduce_sum的理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-stack%E7%9A%84%E7%90%86%E8%A7%A3"><span class="nav-number">2.2.</span> <span class="nav-text">tf.stack的理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B7%E6%9C%89%E5%85%88%E5%90%8E%E9%A1%BA%E5%BA%8F%EF%BC%8Csynchronize%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-number">2.3.</span> <span class="nav-text">具有先后顺序，synchronize的计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#embedding-%E5%92%8C-lookupTable-link"><span class="nav-number">2.4.</span> <span class="nav-text">embedding 和 lookupTable [link]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function"><span class="nav-number">2.5.</span> <span class="nav-text">loss function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92"><span class="nav-number">2.5.1.</span> <span class="nav-text">回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E7%B1%BB"><span class="nav-number">2.5.2.</span> <span class="nav-text">分类</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#tf-softmax-cross-entropy-with-logits"><span class="nav-number">2.5.2.1.</span> <span class="nav-text">tf.softmax_cross_entropy_with_logits</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%83%E8%AF%95"><span class="nav-number">3.</span> <span class="nav-text">调试</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-Print"><span class="nav-number">3.1.</span> <span class="nav-text">tf.Print</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-cond"><span class="nav-number">3.2.</span> <span class="nav-text">tf.cond</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#same-%E5%92%8C-padding"><span class="nav-number">4.</span> <span class="nav-text">same 和 padding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Session-run-a-b-c-%E4%B8%AD%E5%8F%98%E9%87%8F%E7%9A%84%E9%A1%BA%E5%BA%8F"><span class="nav-number">5.</span> <span class="nav-text">Session.run([a,b,c])中变量的顺序</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Session-%E5%92%8C-Graph-%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">6.</span> <span class="nav-text">Session 和 Graph 的关系</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-Function"><span class="nav-number">7.</span> <span class="nav-text">Loss Function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-tf-nn-sigmoid-cross-entropy-with-logits"><span class="nav-number">7.1.</span> <span class="nav-text">1. tf.nn.sigmoid_cross_entropy_with_logits</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-tf-losses-log-loss"><span class="nav-number">7.2.</span> <span class="nav-text">2. tf.losses.log_loss</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%87%BA%E5%92%8C%E6%81%A2%E5%A4%8D"><span class="nav-number">8.</span> <span class="nav-text">模型导出和恢复</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%81%A2%E5%A4%8D"><span class="nav-number">8.1.</span> <span class="nav-text">模型恢复</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#savedModel"><span class="nav-number">8.1.1.</span> <span class="nav-text">savedModel</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensorflow-%E4%B8%BB%E6%B5%81%E7%A8%8B"><span class="nav-number">9.</span> <span class="nav-text">Tensorflow 主流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E9%83%A8%E5%88%86"><span class="nav-number">9.1.</span> <span class="nav-text">梯度更新部分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-Summary"><span class="nav-number">9.2.</span> <span class="nav-text">tf.Summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keras-backend-tensorflow"><span class="nav-number">10.</span> <span class="nav-text">Keras backend tensorflow</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensorboard"><span class="nav-number">11.</span> <span class="nav-text">Tensorboard</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F"><span class="nav-number"></span> <span class="nav-text">分布式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F-1"><span class="nav-number">1.</span> <span class="nav-text">分布式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B3%BB%E7%BB%9F%E4%BC%9A%E5%8C%85%E5%90%AB%E4%B8%89%E7%A7%8D%E7%B1%BB%E5%9E%8B%E7%9A%84%E8%8A%82%E7%82%B9"><span class="nav-number">1.1.</span> <span class="nav-text">系统会包含三种类型的节点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8FTensorFlow%E4%BB%A3%E7%A0%81%E7%9A%84%E5%87%86%E5%A4%87%E5%8C%85%E6%8B%AC%E4%B8%89%E4%B8%AA%E9%98%B6%E6%AE%B5"><span class="nav-number">1.2.</span> <span class="nav-text">分布式TensorFlow代码的准备包括三个阶段</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PS-and-Worker"><span class="nav-number">2.</span> <span class="nav-text">PS and Worker</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Client"><span class="nav-number">2.1.</span> <span class="nav-text">Client</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Job"><span class="nav-number">2.2.</span> <span class="nav-text">Job</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Master-service"><span class="nav-number">2.3.</span> <span class="nav-text">Master service</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Between-graph-replication"><span class="nav-number">3.</span> <span class="nav-text">Between-graph replication</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">45</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
