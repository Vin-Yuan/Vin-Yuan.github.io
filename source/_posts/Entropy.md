---
title: Entropy
date: 2018-05-10 17:15:21
tags: 机器学习 数学
mathjax: true
---
# Cross Entropy

$H(y_i) = \sum_i y_ilog(\frac{1}{y_i}) = -\sum_iy_ilog(y_i)$
---
例子：现在有两枚硬币，抛出有四种情况，正正、正负、负正、负负。如果用熵来计算需要几为表示信息的话，计算如下：

- 2进制
$H(y_i) = 4.({\frac{1}{4}.log_24}) = 2$，即00，01，10，11就可表示
- 4进制
$H(y_i) = 4.({\frac{1}{4}.log_44}) = 1$，即1，2，3，4就可表示

[交叉熵代价函数][1]

交叉熵理论

交叉熵与熵相对，如同协方差与方差。

熵考察的是单个的信息（分布）的期望：


$H(p)=-\sum_{i=1}^n p(x_i)\log p(x_i)$
交叉熵考察的是两个的信息（分布）的期望： 

$H(p,q)=-\sum_{i=1}^np(x_i)\log q(x_i)$

详见 wiki Cross entropy
交叉熵代价函数


$L_H(\mathbf x,\mathbf z)=-\sum_{k=1}^dx_k\log z_k+(1-x_k)\log(1-z_k)$

x 表示原始信号，z 表示重构信号，以向量形式表示长度均为 d，又可轻易地将其改造为向量内积的形式。
神经网络中的交叉熵代价函数

为神经网络引入交叉熵代价函数，是为了弥补 sigmoid 型函数的导数形式易发生饱和（saturate，梯度更新的较慢）的缺陷。

首先来看平方误差函数（squared-loss function），对于一个神经元（单输入单输出），定义其代价函数： 

$C=(a−y)22$

其中 a=σ(z),z=wx+b，然后根据对权值（w）和偏置（b）的偏导（为说明问题的需要，不妨将 x=1,y=0）： 
$∂C∂w=(a−y)σ′(z)x=aσ′(z)∂C∂b=(a−y)σ′(z)=aσ′(z)$
根据偏导计算权值和偏置的更新： 

$w=w−η∂C∂w=w−ηaσ′(z)b=b−η∂C∂b=b−ηaσ′(z)$
无论如何简化，sigmoid 型函数的导数形式 σ′(z) 始终阴魂不散，上文说了 σ′(z) 较容易达到饱和，这会严重降低参数更新的效率。

为了解决参数更新效率下降这一问题，我们使用交叉熵代价函数替换传统的平方误差函数。

对于多输入单输出的神经元结构而言，如下图所示： 

这里写图片描述 
我们将其损失函数定义为： 

C=−1n∑xylna+(1−y)ln(1−a)

其中 a=σ(z),z=∑jwjxj+b
最终求导得： 

∂C∂w=1n∑xxj(σ(z)−y)∂C∂b=1n∑x(σ(z)−y)
就避免了 σ′(z) 参与参数更新、影响更新效率的问题；


  [1]: http://blog.csdn.net/lanchunhui/article/details/50970625
