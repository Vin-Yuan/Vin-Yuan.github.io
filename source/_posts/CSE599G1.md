---
title: CSE599G1
date: 2019-03-28 16:02:42
categories:
tags: deep learning, batch normalization
mathjax: true
---

# lecture 1

在多层神经网络里，大多会有梯度消失和梯度爆炸的现象，如下图：

设想当所有$W$都相同的时候，输入数据 $x$ 在$W > 1$ ，$W < 1$ 会指数级别的变化，导致输入数据被“稀释”or“膨胀”，对于激活函数来说，无论是Sigmoid、tanh 在$x$特别大和特别小的时候梯度会变为0。

![](http://ww1.sinaimg.cn/large/6bf0a364ly1g1ikl4ph78j20w50e5my0.jpg)

<!-- more -->

在网络不断传递过程钟，数据的量级会发生变化，可以通过两种方式改善：

## 1.1 Batch Normalization:

标准化数据可以稳定数据的量级，对输入经行常数缩放，Batch Normalization后输出仍不变。即：$BN(\alpha x)_i = BN(x)_i$。这对于稳定magnitude很有帮助，同时利于调节learning rate，较少参数受初始化的影响（这里我的理解是每一层都对这一batch 的数据经行Batch Normalization，以起到量级magnitude的控制）。



**Batch Normalization就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。**

Covariate Shift :  

Batch Normalization 是用来解决“Internal Covariate Shift”问题的。首先说明Mini-Batch SGD相对于One Example SGD 的两个优势：梯度更新方向更准确；并行计算速度快。

**对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。**

详细可见 

[^1]: 1 https://www.cnblogs.com/guoyaohua/p/8724433.html batch normalization



## 1.2 Residual Net

残差网络也可以解决上面的问题

Instead of doing transformation, it adds transformation result to input.

Partly solve vanishing/explosive value problem.

![](http://ww1.sinaimg.cn/mw690/6bf0a364ly1g1irsm8agsj20gj0afjuo.jpg)

[1]: https://www.cnblogs.com/guoyaohua/p/8724433.htmlbatchnormalization	"batch normalization"



