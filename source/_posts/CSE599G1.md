---
title: CSE599G1
date: 2019-03-28 16:02:42
categories:
tags: deep learning
---

# lecture 1

在多层神经网络里，大多会有梯度消失和梯度爆炸的现象，如下图：

设想当所有$W$都相同的时候，输入数据 $x$ 在$W > 1$ ，$W < 1$ 会指数级别的变化，导致输入数据被“稀释”or“膨胀”，对于激活函数来说，无论是Sigmoid、tanh 在$x$特别大和特别小的时候梯度会变为0。

![](http://ww1.sinaimg.cn/large/6bf0a364ly1g1ikl4ph78j20w50e5my0.jpg)

在网络不断传递过程钟，数据的量级会发生变化，可以通过两种方式改善：

- Batch Normalization:

标准化数据可以稳定数据的量级，对输入经行常数缩放，Batch Normalization后输出仍不变。即：$BN(\alpha x)_i = BN(x)_i$。这对于稳定magnitude很有帮助，同时利于调节learning rate，较少参数受初始化的影响（这里我的理解是每一层都对这一batch 的数据经行Batch Normalization，以起到量级magnitude的控制）。

- Residual Net

残差网络也可以解决上面的问题

Instead of doing transformation, it adds transformation result to input.

Partly solve vanishing/explosive value problem.

![](http://ww1.sinaimg.cn/mw690/6bf0a364ly1g1irsm8agsj20gj0afjuo.jpg)