---
title: svm 随时感想
mathjax: true
date: 2019-07-16 09:28:31
categories:
tags: svm, machine learning

---



以前常会疑惑：
$$
\begin{equation}
w^T\cdot x + b = 0 \tag{1}
\end{equation}
$$


 为什么他可以确定一条直线，以及为什么其作为分割面后 > 0 和 <0 就可以作为分类？

首先我们考虑如何确定一条直线，给定一个法向量$w$，会有无数个直线与其垂直正交，我们要的那一条如何唯一表示呢？其实很简单，找一个点就行，只需要这一个点，再加这一个法向量，一条直线就完全确定了。

假设  $a = (a_1, a_2，a_3)^T$ 是三维空间的一个点:
$$
w^T\cdot (x-a) = 0 \tag{2}
$$

可以确定一条直线，这是两个向量的乘积。$w$和$a$都是常量，所以展开后会生成一个常数项，即（1）式的 $b$， 最后形式就是 (1)。

值得注意的是：n维空间的一个分割超平面是n-1维的，减少了一维的降维打击。即3维立体空间：分割面为2维平面；2维平面：分割面为一维直线，1维直线：分个面为一个点。例如：$y = kx + b$是一维的，原因是$y$受$x$控制。$ax + by + cz = 0$是2维的是因为任选一维都是受另外两维控制，非自由的，这一点和线性代数的最大无关向量组很像。

接上面的说，对于式子（1），其确定一条直线，两边的点带入要么大于零，要么小于零，直观去想为什么呢？

其实很简单，对于给定点$a$， 所有**基于a的向量**，可以分为三类：

- 与$w^T$相乘等于零的，过a点且垂直与法向量$w$

- 与$w^T$相乘小于零的，过a点且与法向量$w$夹角小于90度的，比如说postive sample
- 与$w^T$相乘大于零的，过a点且与法向量$w$夹角大于90度的，比如说negative sample

这样就很直观明显了。

## Hinge Loss

![](http://ww1.sinaimg.cn/mw690/6bf0a364ly1g57kcedt3oj20bp08yjrr.jpg)

在机器学习中，**hinge loss**作为一个损失函数(loss function)，通常被用于最大间隔算法。在SVM定义损失函数
$$
\ell(y) = \max(0, 1-y \cdot \hat y)
$$

定义这样的损失函数是因为svm算法有一个需求，**样本被分对且离分割面越远越好**。

对于分类正确且远离分割面的样本，我们希望损失贡献最小或为0；而对于分类器难以分辨的样本，我们希望其损失贡献最大。基于这些需求，我们采用了hinge loss function.