---
title: Normlization
mathjax: true
date: 2025-05-19 12:05:44
categories:
tags: deeplearning LLM
---

# Layer Normalization


éªŒè¯LayerNormçš„ï¼Œé€šè¿‡ä½¿ç”¨torch.meanå’Œtorch.varå¤ç°çš„æ—¶å€™å‘ç°ä¸ä¸€è‡´
LayerNormé»˜è®¤ä½¿ç”¨çš„æ˜¯biasçš„æ•´ä½“æ–¹å·®, divided by N
torch.varé»˜è®¤ä½¿ç”¨çš„æ˜¯æ— biasçš„æ ·æœ¬æ–¹å·®, devided by N-1


å¯¹äºæ¯ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾å‘é‡ $x \in \mathbb{R}^d$ ï¼ŒLayerNorm æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

$$
\operatorname{LayerNorm}(x)=\gamma \cdot \frac{x-\mu}{\sqrt{\sigma^2+\epsilon}}+\beta
$$

- $\mu, \sigma^2$ ï¼šå½“å‰æ ·æœ¬çš„å‡å€¼å’Œæ–¹å·®ï¼ˆä»…ç”¨äºå½’ä¸€åŒ–ï¼‰
- $\gamma$ ï¼šå¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼ˆscaleï¼Œç±»ä¼¼äºæƒé‡ï¼‰
- $\beta$ ï¼šå¯å­¦ä¹ çš„åç§»å‚æ•°ï¼ˆbiasï¼Œåç½®ï¼‰
  
[![LayerNomr](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0257ddec-b348-41d7-905d-5bc2b54fd557_1280x720.png)](https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)

<!-- more -->

å¯¹äºè¿™ä¸¤ä¸ªåç§»å‚æ•°å¦‚ä½•æ›´æ–°ï¼Œæ¢ç©¶äº†ä¸€ä¸‹åº•å±‚å®ç°
```python
import torch
import torch.nn as nn
import torch.optim as optim

layer_norm = nn.LayerNorm(2)
x = torch.tensor([[1.0, 2.0], [2.0, 3.0]], requires_grad=True)
target = torch.tensor([[0.0, 0.0], [0.0, 0.0]])  # ç›®æ ‡å…¨ä¸º 0

optimizer = optim.SGD(layer_norm.parameters(), lr=0.1)

for i in range(3):
    optimizer.zero_grad()
    out = layer_norm(x)
    loss = ((out - target)**2).mean()
    loss.backward()
    optimizer.step()
    
    print(f"Step {i}, beta: {layer_norm.bias.data}")

```

$\gamma$ å’Œ $\beta$ çš„ç»´åº¦æ˜¯ (batch_size, seq_len)
$\sigma$ ä»¥åŠ $\mu$ çš„ç»´åº¦æ˜¯ (batch_size, seq_len, feature_dim), åœ¨å®Œæˆæ ‡å‡†åŒ–åï¼Œè¿™ä¸¤ä¸ªä¼šä»¥å‘é‡çš„å½¢å¼
ä¾‹å¦‚:
```python
x = [[1.0, 2.0, 3.0, 4.0],
     [5.0, 6.0, 7.0, 8.0]]

LayerNorm ä¼šå¯¹æ¯ä¸€è¡Œåšæ ‡å‡†åŒ–ï¼š
=> æ¯è¡Œå˜æˆå‡å€¼=0ï¼Œæ–¹å·®=1 çš„å‘é‡

gamma, beta = [Î³1, Î³2, Î³3, Î³4], [Î²1, Î²2, Î²3, Î²4]

æœ€åè¾“å‡º = normalized * gamma + beta
```
åº•å±‚åœ¨ç›¸ä¹˜çš„æ—¶å€™ï¼Œä¸€èˆ¬ä¼šå‘é‡åŒ–
$ \hat{x} * {\gamma}^{T} + \beta$
å…¶ä¸­ $\hat{x} ,\gamma, \beta \in \mathbb{R}^d$

$\hat{x} = \frac{x-\mu}{\sqrt{\sigma^2}}$
$ y = \hat{x} \cdot \gamma + \beta $
å¯¹äº$\beta$, åå‘ä¼ æ’­çš„æ¢¯åº¦ï¼š $\frac{\partial L}{\partial \beta}=\frac{\partial L}{\partial y}$
å¯¹äº$\gamma$, åå‘ä¼ æ’­æ¢¯åº¦ï¼š $\frac{\partial L}{\partial \gamma}=\frac{\partial L}{\partial y} \cdot \hat{x}$

é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆ Î³ å’Œ Î² è¦å…±äº«ï¼Ÿ
### 1. å½’ä¸€åŒ–åä¸¢å¤±äº†å°ºåº¦å’Œåç§»ä¿¡æ¯ï¼Œéœ€è¦ Î³ã€Î² æ¥æ¢å¤è¡¨è¾¾èƒ½åŠ›
å½’ä¸€åŒ–æœ¬è´¨ä¸Šæ˜¯æŠŠæ•°æ®å˜æˆäº†é›¶å‡å€¼ã€å•ä½æ–¹å·®ã€‚

è¿™æ ·è™½ç„¶æœ‰åŠ©äºç¨³å®šè®­ç»ƒï¼Œä½†ä¼šæŸå¤±ä¸€äº›ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼Œâ€œè¿™ä¸ªç¥ç»å…ƒåŸæœ¬è¾“å‡ºå¾ˆå¤§æ˜¯æœ‰æ„ä¹‰çš„â€ï¼‰ã€‚

æ‰€ä»¥é€šè¿‡å¼•å…¥ Î³ å’Œ Î² è¿™ä¸¤ä¸ª å¯å­¦ä¹ å‚æ•°ï¼Œç½‘ç»œå¯ä»¥åœ¨è®­ç»ƒä¸­å­¦ä¹ â€œæ˜¯å¦éœ€è¦æ”¾å¤§æŸäº›ç»´åº¦â€æˆ–â€œæ•´ä½“å¹³ç§»â€ï¼Œä»¥æ¢å¤è¿™ç§è¡¨è¾¾èƒ½åŠ›ã€‚

ğŸ¯ å…³é”®ç‚¹ï¼šÎ³ å’Œ Î² æ˜¯ æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸æ˜¯ç”¨æ¥é€‚é…æ¯ä¸ªæ ·æœ¬ï¼Œè€Œæ˜¯å­¦ä¹ ä¸€ç§åœ¨æ‰€æœ‰æ ·æœ¬ä¸Šéƒ½æœ‰æ•ˆçš„å˜æ¢æ–¹å¼ï¼Œè¿™ç¬¦åˆæ·±åº¦å­¦ä¹ æ¨¡å‹â€œå…±äº«å‚æ•°â€çš„ç†å¿µã€‚
### 2. ä¸å¯¹æ¯ä¸ªæ ·æœ¬å•ç‹¬å­¦ä¹  Î³ å’Œ Î² æ˜¯ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆ + ä¿æŒå‚æ•°æ•ˆç‡
å¦‚æœ Î³ å’Œ Î² å¯¹æ¯ä¸ªæ ·æœ¬éƒ½æœ‰ç‹¬ç«‹çš„ä¸€å¥—ï¼Œé‚£æ„å‘³ç€å‚æ•°é‡å°†éšç€ batch size æˆå€å¢é•¿ã€‚

è¿™ä¼šï¼š

å¤§å¹…å¢åŠ è®¡ç®—å’Œå†…å­˜è´Ÿæ‹…

ç ´åæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ˆç›¸å½“äºä¸ºæ¯ä¸ªæ ·æœ¬å®šåˆ¶å½’ä¸€åŒ–ï¼Œå¯èƒ½ä¼šè¿‡æ‹Ÿåˆï¼‰

æ‰€ä»¥ï¼šå…±äº« Î³ å’Œ Î² æ˜¯ä¸€ç§åœ¨ä¿æŒæ¨¡å‹è¡¨è¾¾èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚

### 3. ä¸ BatchNorm çš„åŒºåˆ«ä¹Ÿä½“ç°å‡ºè¿™ç§è®¾è®¡å“²å­¦
BatchNorm ä½¿ç”¨çš„æ˜¯ batch å†…çš„ç»Ÿè®¡é‡ï¼ˆè·¨æ ·æœ¬ç»Ÿè®¡ï¼‰ï¼Œé€‚ç”¨äºå›¾åƒç­‰åŒåˆ†å¸ƒæ ·æœ¬ã€‚

LayerNorm ä½¿ç”¨çš„æ˜¯ æ ·æœ¬å†…çš„ç»Ÿè®¡é‡ï¼Œé¿å…ä¾èµ– batch å¤§å°ï¼ˆé€‚åˆ Transformer è¿™ç§åºåˆ—å»ºæ¨¡ï¼‰ã€‚

ä½†æ— è®ºå“ªç§å½’ä¸€åŒ–ï¼ŒÎ³ å’Œ Î² å§‹ç»ˆæ˜¯å…±äº«çš„å‚æ•°ï¼Œå› ä¸ºå®ƒä»¬æ˜¯æ¨¡å‹æœ¬èº«çš„ä¸€éƒ¨åˆ†ï¼Œä¸ä¾èµ–äºè¾“å…¥æ ·æœ¬ã€‚

ğŸ§ª ä¸¾ä¸ªæ¯”å–»ï¼š
ä½ æœ‰ä¸€ä¸ªå½’ä¸€åŒ–åçš„å›¾åƒæ•°æ®é›†ï¼Œæ¯å¼ å›¾éƒ½è¢«æ ‡å‡†åŒ–æˆäº®åº¦ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ã€‚ä½†ä½ çŸ¥é“æœ‰äº›å›¾åƒæœ¬è¯¥äº®ä¸€äº›ã€æœ‰äº›æœ¬è¯¥æš—ä¸€äº›ã€‚äºæ˜¯ä½ è®­ç»ƒä¸€ä¸ªâ€œäº®åº¦å¢ç›Šâ€å’Œâ€œäº®åº¦åç§»â€å‚æ•°ï¼Œç”¨æ¥ç»Ÿä¸€åœ°è°ƒæ•´æ‰€æœ‰å›¾åƒã€‚ä½ ä¸ä¼šä¸ºæ¯å¼ å›¾å­¦ä¸€ä¸ªå¢ç›Šï¼Œè€Œæ˜¯æ‰¾å‡ºä¸€ç»„å¯¹æ‰€æœ‰å›¾éƒ½é€‚ç”¨çš„å‚æ•°ã€‚

âœ… æ€»ç»“ï¼š
é—®é¢˜	è§£é‡Š
ä¸ºä»€ä¹ˆ Î³ å’Œ Î² è¦ batch å…±äº«ï¼Ÿ	å› ä¸ºå®ƒä»¬æ˜¯æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºæ¢å¤è¡¨è¾¾èƒ½åŠ›ï¼Œä¸æ˜¯è¾“å…¥çš„ä¸€éƒ¨åˆ†ï¼›å…±äº«å¯ä»¥å‡å°‘å‚æ•°é‡ã€é¿å…è¿‡æ‹Ÿåˆ
ä¸ºä»€ä¹ˆä¸å¯¹æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å­¦ä¹  Î³ å’Œ Î²ï¼Ÿ	è¿™æ ·ä¼šå¤§å¤§å¢åŠ å‚æ•°ã€å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”ä¸ç¬¦åˆæ·±åº¦å­¦ä¹ â€œå‚æ•°å…±äº«â€çš„æ ¸å¿ƒè®¾è®¡å“²å­¦
Î³ å’Œ Î² çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ	æ¢å¤å½’ä¸€åŒ–è¿‡ç¨‹ä¸¢å¤±çš„å°ºåº¦å’Œåç§»ä¿¡æ¯ï¼Œä½¿æ¨¡å‹ä¿ç•™å­¦ä¹ èƒ½åŠ›


# Batch Normliazation

Batch Normalizationï¼ˆæ‰¹é‡å½’ä¸€åŒ–ï¼‰ä¸­çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µï¼šè¿è¡Œç»Ÿè®¡é‡ï¼ˆrunning statisticsï¼‰ çš„æ›´æ–°å’Œä½¿ç”¨ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒBatch Normalization ä¼šè®¡ç®—æ¯ä¸ªæ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®ï¼Œå¹¶ç”¨è¿™äº›ç»Ÿè®¡é‡æ¥å½’ä¸€åŒ–å½“å‰æ‰¹æ¬¡çš„æ•°æ®ã€‚ç„¶è€Œï¼Œè¿™äº›æ‰¹æ¬¡å†…çš„ç»Ÿè®¡é‡å¹¶ä¸ç›´æ¥ç”¨äºæœ€ç»ˆçš„å½’ä¸€åŒ–ï¼Œè€Œæ˜¯ç”¨æ¥æ›´æ–°è¿è¡Œç»Ÿè®¡é‡ï¼Œè¿™äº›è¿è¡Œç»Ÿè®¡é‡ä¼šåœ¨æ¨ç†ï¼ˆinferenceï¼‰é˜¶æ®µä½¿ç”¨ã€‚

```python
import torch
import torch.nn as nn

class CustomBatchNorm(nn.Module):
    def __init__(self, num_features, momentum=0.1, eps=1e-5):
        super().__init__()
        self.momentum = momentum
        self.eps = eps
        self.running_mean = torch.zeros(num_features)
        self.running_var = torch.ones(num_features)
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))

    def forward(self, x):
        if self.training:
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®
            batch_mean = x.mean(dim=0)
            batch_var = x.var(dim=0, unbiased=False)
            # æ›´æ–°è¿è¡Œç»Ÿè®¡é‡
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var
            # å½’ä¸€åŒ–å½“å‰æ‰¹æ¬¡çš„æ•°æ®
            x_norm = (x - batch_mean) / torch.sqrt(batch_var + self.eps)
        else:
            # ä½¿ç”¨è¿è¡Œç»Ÿè®¡é‡å½’ä¸€åŒ–æ•°æ®
            x_norm = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)
        # åº”ç”¨ç¼©æ”¾å’Œåç§»
        return self.gamma * x_norm + self.beta
```